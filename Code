#Importing Nivea UK file
import pandas as pd
import sapdi
ws = sapdi.get_workspace(name='Hackathon')
dc = ws.get_datacollection(name='01_Rating_and_Review')
with dc.open('/unsupervised/nivea_uk.csv').get_reader() as reader:
    df = pd.read_csv(reader)

# Nivea_uk file
df

# Importing nivea_us file
import pandas as pd
import sapdi
ws = sapdi.get_workspace(name='Hackathon')
dc = ws.get_datacollection(name='01_Rating_and_Review')
with dc.open('/unsupervised/nivea_us.csv').get_reader() as reader:
    df1 = pd.read_csv(reader)

# Nivea us file
df1


# Reviews from Amazon
import pandas as pd
import sapdi
ws = sapdi.get_workspace(name='Hackathon')
dc = ws.get_datacollection(name='01_Rating_and_Review')
with dc.open('/unsupervised/nivea_uk_amazon.csv').get_reader() as reader:
    df2 = pd.read_csv(reader)

# Merging the 3 files
dataframe = pd.concat([df,df1,df2])

# Dataframe with us,uk and amazon data
dataframe

#Installing nltk lib
pip install -U --user nltk

# Loading the required components
import nltk
nltk.download()

# Stopwords function 
from nltk.corpus import stopwords

# Stem function
from nltk.stem import PorterStemmer

# Defining the stop words
stopwords=stopwords.words('english')

# Examining the stop words
print(stopwords)

# Creating a column of reviews without stopwords
dataframe['without_stopwords'] = dataframe['content'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords)]))

# Examining the created dataframe
Dataframe




# Stemming the review content
ps = PorterStemmer()
dataframe['stemmed_content'] = dataframe['without_stopwords'].apply(lambda x: ' '.join([ps.stem(word) for word in x.split()]))

# Examining the created datframe
dataframe

# Importing nltk library for lemmetization
import nltk
nltk.download('wordnet')

# Lemmetizing the reviews
from nltk.stem.wordnet import WordNetLemmatizer
lmtzr = WordNetLemmatizer()
dataframe['lemmatized_text'] = dataframe['without_stopwords'].apply(lambda x: ' '.join([lmtzr.lemmatize(word,'v') for word in x.split() ]))

# Examining the dataframe
dataframe

# Tokenize function
from nltk import tokenize

import nltk
nltk.download('punkt')

# Tokenizing the lemmatized reviews
dataframe['tokenized_text'] = dataframe.apply(lambda row: nltk.sent_tokenize(row['lemmatized_text']), axis=1)

dataframe

pip install --user -U gensim

#  Loading gensim library and functions for LDA
import gensim
from gensim import corpora

# Creating tokenized words for matrix to be included in LDA
dataframe['tokenized_words'] = pd.Series(dataframe['lemmatized_text']).apply(lambda x: x.split())
dataframe
# Defining a dictionary to store tokenized words
dictionary = corpora.Dictionary(dataframe['tokenized_words'])

# Creating a matrix
doc_term_matrix = [dictionary.doc2bow(rev) for rev in dataframe['tokenized_words']]

# Creating the object for LDA model using gensim library
LDA = gensim.models.ldamodel.LdaModel

# Building LDA model
lda_model = LDA(corpus=doc_term_matrix, id2word=dictionary, num_topics=10, random_state=100,
                chunksize=1000, passes=50)

# Exploring the LDA model topics
lda_model.print_topics()

# Installing libraries for visualizing the topics
pip install pyldavis

# Visualization of the topics generated by LDA model
import pyLDAvis.gensim
pyLDAvis.enable_notebook()
vis = pyLDAvis.gensim.prepare(lda_model, doc_term_matrix, dictionary)
vis

#Sentiment analysis on each review
import nltk
nltk.download('vader_lexicon')
from nltk.sentiment.vader import SentimentIntensityAnalyzer
analyzer = SentimentIntensityAnalyzer()
sentences = dataframe['lemmatized_text'].to_list()
for sentence in sentences:
    senti = analyzer.polarity_scores(sentence)
    print(sentence)
    print(senti,'\n')
    if senti['compound'] >= 0.05 : 
        print("Positive",'\n') 
    elif senti['compound'] <= - 0.05 : 
        print("Negative",'\n') 
    else : 
        print("Neutral",'\n')

# Sentiment analysis on each sentence in review
import nltk
nltk.download('vader_lexicon')
from nltk.sentiment.vader import SentimentIntensityAnalyzer
analyzer = SentimentIntensityAnalyzer()
sentences = dataframe['tokenized_text']
sentences1 = sentences.explode()
for sentence in sentences1:
    senti = analyzer.polarity_scores(sentence)
    print(sentence)
    print(senti,'\n')
    if senti['compound'] >= 0.05 : 
        print("Positive",'\n') 
    elif senti['compound'] <= - 0.05 : 
        print("Negative",'\n') 
    else : 
        print("Neutral",'\n')

# Sentiment analysis on each review in datframe

dataframe1 = dataframe
from nltk.sentiment.vader import SentimentIntensityAnalyzer
analyzer = SentimentIntensityAnalyzer()

def get_sentiment(row, **kwargs):
    sentiment_score = analyzer.polarity_scores(row)
    positive_meter = round((sentiment_score['pos'] * 10), 2)
    negative_meter = round((sentiment_score['neg'] * 10), 2)
    return positive_meter if kwargs['k'] == 'positive' else negative_meter
    
dataframe1['positive'] = dataframe1.lemmatized_text.apply(get_sentiment, k='positive')
dataframe1['negative'] = dataframe1.lemmatized_text.apply(get_sentiment, k='negative')
    
    
# Creating a column for sentiment analysis based on positive and negative review
dataframe1.loc[dataframe1['positive'] < dataframe1['negative'], 'Sentiment'] = 'Negative' 
dataframe1.loc[dataframe1['positive'] > dataframe1['negative'] , 'Sentiment'] = 'Positive'
dataframe1.loc[dataframe1['positive'] == dataframe1['negative'] , 'Sentiment'] = 'Neutral'

# Final dataframe
dataframe1.head()

# Sentiment analysis for each sentence in review in dataframe

dataframe2 = dataframe.explode('tokenized_text')
from nltk.sentiment.vader import SentimentIntensityAnalyzer
analyzer = SentimentIntensityAnalyzer()

def get_sentiment(row, **kwargs):
    sentiment_score = analyzer.polarity_scores(row)
    positive_meter = round((sentiment_score['pos'] * 10), 2)
    negative_meter = round((sentiment_score['neg'] * 10), 2)
    return positive_meter if kwargs['k'] == 'positive' else negative_meter
    
dataframe2['positive'] = dataframe2.tokenized_text.apply(get_sentiment, k='positive')
dataframe2['negative'] = dataframe2.tokenized_text.apply(get_sentiment, k='negative')
    
    
# Creating a column for sentiment analysis based on positive and negative review
dataframe2.loc[dataframe2['positive'] < dataframe2['negative'], 'Sentiment'] = 'Negative' 
dataframe2.loc[dataframe2['positive'] > dataframe2['negative'] , 'Sentiment'] = 'Positive'
dataframe2.loc[dataframe2['positive'] == dataframe2['negative'] , 'Sentiment'] = 'Neutral'

# Final dataframe
dataframe2.head()
